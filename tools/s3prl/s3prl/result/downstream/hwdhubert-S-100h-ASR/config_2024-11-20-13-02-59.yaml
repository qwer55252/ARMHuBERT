downstream_expert:
  datarc:
    batch_size: 32
    bucket_file: ../../../data/len_for_bucket
    decoder_args:
      beam: 5
      beam_threshold: 25
      criterion: ctc
      decoder_type: None
      kenlm_model: ../../datasets/4-gram.arpa
      lexicon: ../../datasets/librispeech_lexicon.lst
      lm_weight: 2
      nbest: 1
      sil_weight: 0
      unk_weight: -math.inf
      word_score: -1
    dev-clean:
    - dev-clean
    dev-other:
    - dev-other
    dict_path: ./downstream/asr/char.dict
    eval_batch_size: 1
    libri_root: /DB/LibriSpeech/LibriSpeech
    num_workers: 12
    test-clean:
    - test-clean
    test-other:
    - test-other
    train:
    - train-clean-100
    train_batch_size: 32
    zero_infinity: true
  modelrc:
    RNNs:
      bidirection: true
      dim:
      - 1024
      - 1024
      dropout:
      - 0.2
      - 0.2
      layer_norm:
      - false
      - false
      module: LSTM
      proj:
      - false
      - false
      sample_rate:
      - 1
      - 1
      sample_style: concat
      total_rate: -1
    Wav2Letter:
      total_rate: 320
    project_dim: 1024
    select: RNNs
optimizer:
  lr: 0.0001
  name: TorchOptim
  torch_optim_name: Adam
runner:
  eval_dataloaders:
  - dev-clean
  eval_step: 2000
  gradient_accumulate_steps: 1
  gradient_clipping: 1
  log_step: 100
  max_keep: 1
  save_step: 500
  total_steps: 200000
scheduler:
  name: linear_schedule_with_warmup
  num_warmup_steps: 1400
specaug:
  adaptive: false
  adaptive_number_ratio: 0.04
  adaptive_size_ratio: 0.04
  apply_freq_mask: true
  apply_time_mask: true
  apply_time_warp: true
  freq_mask_width_range:
  - 0
  - 50
  max_n_time_masks: 20
  num_freq_mask: 4
  num_time_mask: 2
  time_mask_width_range:
  - 0
  - 40
  time_warp_window: 5
